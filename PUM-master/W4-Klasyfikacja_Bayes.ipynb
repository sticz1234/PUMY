{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja Bayesowska - zmienne skalarne (1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatywnym sposobem podejścia do problemu klasyfikacji próbki jest przyjęcie, że jej podobieństwo do klasy będzie wyrażone ilościowo poprzez określenie prawdopodobieństwa przynależności do tej klasy. Reguła klasyfikacji przybierze wtedy intuicyjną formę: próbka należy do najbardziej prawdopodobnej klasy. Zakładając, że mamy dwie klasy, np. 'PIES' i 'KOT', a nieznana próbka to x, oraz że prawdopodobieństwa przynależności próbki do klas oznaczymy odpowiednio jako $p(PIES/x)$ i $p(KOT/x)$ (czytaj: prawdopododobieństwo stwierdzenia klasy '...' pod warunkiem, że mamy daną próbkę x), można to wyrazić formalnie, w postaci reguły:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "x = \\huge\\{ \\normalsize\n",
    "\\begin{array}{lll} PIES & if & p(PIES/x) > p(KOT/x) \\\\   KOT & if & p(PIES/x) < p(KOT/x) \n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby możliwe było wyznaczenie wyniku przedstawionej reguły, punktom przestrzeni stanowiącej dziedzinę zadania muszą być przypisane prawdopodobieństwa przynależności do rozważanych klas. Prawdopodobieństwa te będą określane na podstawie położeń próbek zbioru treningowego, przy czym podobnie jak w przypadku klasyfikacji minimalnoodległościowej, możliwe będzie przyjęcie podejścia parametrycznego lub nieparametrycznego. W pierwszym przypadku, zakładany będzie pewien funkcyjny, parametryczny rozkład prawdopodobieństwa, a wartości parametrów będą określane na podstawie dostępnych próbek treningowych. W drugim przypadku, prawdopodobieństwa punktom przestrzeni będą przypisywane jako wypadkowa 'oddziaływań' próbek treningowych, umieszczanych w tej przestrzeni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozkład Gaussa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wspomniane 'oddziaływania' muszą mieć postać funkcyjną, tak by każdemu punktowi przestrzeni jednoznacznie przypisać prawdopodobieństwo przynależności do klasy. Jeżeli nie istnieją inne przesłanki, sensowne jest wyrażenie siły oddziaływania próbki treningowej jako funkcji odległości od tej próbki - im dalej, tym słabsze. Tak określone zależności nazywają się $\\textit{funkcjami radialnymi}$ (radius = promień). Każda funkcja, której argument będzie miał postać $||x-m||$, gdzie $x$ to zmienna, $m$ - parametr (np. wartość próbki treningowej) a symbol $||...||$ oznacza odległość, będzie funkcją radialną. Jedną z najpowszechniej stosowanych funkcji radialnych (między innymi z uwagi na jej 'dopasowanie' do modelowania zjawisk natury) jest funkcja Gaussa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-(\\frac{x-m}{\\sigma})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gdzie parametr $\\sigma$ opisuje szybkość opadania wstęg bocznych funkcji. Załóżmy, że dany jest punkt o współrzędnej $m=2$ - odpowiednia funkcja Gaussa jest generowana za pomocą następującego kodu (proszę zaobserwować zmiany kształtu funkcji, gdy zmienia się parametr $\\sigma$, ustalony w przykładzie na wartość 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# wykresy będą się pojawiać w tekście dokumentu\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Gauss_pdf(x,m,s) :\n",
    "    # utworzenie wektora o identycznych wartościach (m) i długości takiej jak zmienna x\n",
    "    A = m*np.ones(len(x))\n",
    "    return (1/np.sqrt(2*np.pi*s))*np.exp(-((x-A)**2)/s**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utworzenie zbioru argumentów, dla których określana będzie funkcja (dziedzina)\n",
    "x = np.arange(-10,10,0.1)\n",
    "# utworzenie wektora y, zawierającego wartości prawdopodobieństw dla wszystkich argumentów\n",
    "m = 2\n",
    "s = 1 # arbitralne założenie\n",
    "y = Gauss_pdf(x,m,s)\n",
    "#plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zamiast pisać własną funkcję, obliczającą wartości rozkładu Gaussa, można skorzystać z gotowych narzędzi. Gaussowska funkcja gęstości rozkładu prawdopodobieństwa jest zdefiniowana w bibliotece $\\tt{scipy}$, w jej module $\\tt{stats}$, w obiekcie o nazwie $\\tt{norm}$ (od rozkładu normalnego - rozkład normalny to rozkład Gaussa przekształcony tak, by wartości parametrów wynosiły odpowiednio $m=0$ i $s=1$). Sposób generacji rozkładu przedstawionego wcześniej z wykorzystaniem tej funkcji jest następujący:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "y2 = norm.pdf(x,m,s) # x - argument, m,s - parametry\n",
    "#plt.plot(x,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nieparametryczne modele klas  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem postępowania jest przypisanie każdemu punktowi przestrzeni prawdopodobieństwa przynależności do klasy - p-stwa te można uznać za modele klas. Dla rozważanego przykładu będą nas interesować dwa rozkłady p-stw: $p(x/PIES)$ i $p(x/KOT)$ Załóżmy, że dane są dwa przykłady (to jest nasza wiedza, pochodząca ze zbioru treningowego), będące reprezentantami dwóch klas : (40, PIES) i (8, KOT). Ta wiedza ma stanowić podstawę do przypisania punktom rozważanej przestrzeni 1D prawdopodobieństw. Sensowną podstawą przypisywania punktom przestrzeni wartości prawdopodobieństw będzie użycie funkcji Gaussa: punkt odpowiadający próbce KOT (x=8) będzie miał maksymalne p-stwo przynależności do klasy 'Kot' ($p(8)=1$), im dalej, tym p-stwa będą mniejsze (analogicznie sprawa wygląda dla p-stw przynależności do klasy PIES).  Gęstości p-stw przynależności do obydwu klas zostały zaznaczone na poniższym wykresie (gdzie przyjąłem arbitralnie wartość parametru $\\sigma = 2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0,100,500) # alternatywny sposób utworzenia wektora argumentów (punktów jest 500)\n",
    "P = norm.pdf(x,40,2)\n",
    "K = norm.pdf(x,8,2)\n",
    "#plt.plot(x,P, color='blue')\n",
    "#plt.plot(x,K, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każdemu punktowi przestrzeni przypisana została wartość prawdopodobieństwa (określone zostały modele klas), a jeżeli chcielibyśmy dla tak zdefiniowanej przestrzeni p-stw dokonać klasyfikacji nieznanej próbki $a=20$, otrzymamy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kot\n"
     ]
    }
   ],
   "source": [
    "a = 20\n",
    "if norm.pdf(a,40,2) > norm.pdf(a,8,2) :\n",
    "    print('Pies')\n",
    "else :\n",
    "    print('Kot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykład klasy 'Pies' i przykład klasy 'Kot' są wykorzystane do budowy modeli tych klas, wykorzystujących funkcje Gaussa. Mimo, że przyjęta dla modelowania rozkładów p-stw funkcja ma parametry, nie przeprowadzamy żadnej procedury ich estymacji - pierwszy z nich staje się wartością przykładowej próbki, a drugi wybierany jest arbitralnie. Dlatego też, przedstawiona koncepcja klasyfikacji należy do grupy metod $\\textbf{nieparametrycznych}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Znajomość tylko jednego przykładu klasy stanowi słabą podstawę dla uzyskania dobrej klasyfikacji nieznanych obiektów: im więcej będzie przykładów, tym lepsze będą modele. Załóżmy, że mamy dodatkowo po dwa przykłady dla klasy 'Pies' ($P_2=7$ i $P_3=25$) oraz 'Kot' ($K_2=4$ i $K_3=7$). Zakładając, że przykłady były wybierane niezależnie od siebie, każdy z nich ma takie same znaczenie, więc można z każdym z nich skojarzyć funkcję, o takiej samej 'wadze'. Ponieważ łączne p-stwo dla wszystkich możliwych punktów dziedziny musi wynieść 1, 'waga' każdej z funkcji Gaussa, przypisanej do kolejnej próbki musi wynosić 1/3 (jeżeli punktów byłoby n, waga wyniosłaby 1/n). Modele klas 'Pies' i 'Kot', uzyskane dla nowego zbioru przykładów, są następujące:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P = (norm.pdf(x,40,2) + norm.pdf(x,7,2) + norm.pdf(x,25,2))/3\n",
    "K = (norm.pdf(x,8,2) + norm.pdf(x,4,2) + norm.pdf(x,7,2))/3\n",
    "#plt.plot(x,P, color='blue')\n",
    "#plt.plot(x,K, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak łatwo zauważyć, wynik klasyfikacji nieznanej próbki $a=20$ będzie tym razem inny - zostanie ona uznana za przedstawiciela klasy 'Pies'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametryczne modele klas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobnie jak to miało miejsce w przypadku klasyfikacji minimalnoodległościowej, podejmowanie decyzji gdy modele klas są zbudowane na podstawie dużej liczby przykładów, staje się złożone obliczeniowo. Dodatkowo, ponieważ przypisywane próbkom rozkłady mają arbitralnie przyjęty parametr $\\sigma$, uzyskiwane modele również są arbitralne w obszarach, gdzie nie było próbek. Dlatego też, podobnie jak poprzednio, wprowadzono koncepcję parametrycznego sposobu reprezentacji klas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeżeli założymy, że rozważane przez nas klasy zawierają obiekty, które są różnymi realizacjami tej samej ukrytej 'koncepcji' (na przykład, wszystkie obiekty, tak jak to ma miejsce w przypadku psów i kotów, są budowane na bazie schematu zawartego w DNA, ale w szczegółach różnią się w wyniku różnych warunków istniejących w czasie formowania się organizmów), to wspomniana już funkcja Gaussa staje się najlepszym modelem opisującym taką klasę. Wynika to z fundamentalnego twierdzenia statystyki - Centralnego Twierdzenia Granicznego (ang. $\\textit{Central Limit Theorem}$), które mówi, że wynik agregacji zmiennych o dowolnych rozkładach ma rozkład normalny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oznacza to, że dla klas o rozważanych właściwościach, modelem klasy będzie $\\textbf{pojedyncza}$ funkcja Gaussa. W konsekwencji, parametry tej funkcji będą określane na podstawie wszystkich próbek treningowych danej klasy. Podstawa dla estymacji wartości tych parametrów ($\\sigma$ i $m$) jest bardzo intuicyjna. Jeżeli wartość argumentu funkcji jest równa $m$ to wartość prawdopodobieństwa przynależności do klasy jest maksymalna. Najbardziej prawdopodobne jest coś, co jest najbardziej typowe, co jest pewnym 'ideałem', od którego występują odstępstwa. W matematyce, wyznaczenie najbardziej typowej wartości to obliczenie <font color=blue> $\\textbf{wartości średniej}$</font>: estymacja parametru $m$ (oznaczanego od teraz zgodnie z przyjętą konwencją literą $\\mu$) na podstawie zbioru $n$-próbek, będzie  więc obliczeniem wartości średniej próbek obydwu klas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\mu^p = \\frac{1}{n_p}\\sum _{i=1}^{n_p} P_i \\hspace{1cm} \\mu^p = \\frac{1}{n_k} \\sum _{i=1}^{n_k} K_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drugi parametr funkcji Gaussa ($\\sigma$) decyduje o jej 'rozpiętości' - im jest większy, tym bardziej prawdopodobne staje się występowanie wartości bardziej odległych od wartości średniej, im mniejszy, tym mniej. Oznacza to, że parametr ten może być uznany za miarę średniego odstępstwa (odległości) od wartości średniej. W matematyce, miara średniego odstępstwa od wartości średniej, czyli $\\sigma$ to <font color=blue> $\\textbf{odchylenie standardowe}$</font>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma^2 = E(x - \\mu)^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Występujący we wzorze kwadrat odchylenia standardowego nazywa się <font color=blue> $\\textbf{wariancją}$</font>. Podsumowując, parametry modeli klas - funkcji Gaussa, określających prawdopobieństwa przypisywane punktom przestrzeni, są określane na podstawie próbek treningowych tych klas, zgodnie z podanymi powyżej wzorami. Dla rozważanego przykładu, modelami klas będą dwie funkcje:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ p^P(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_P}} e^{-(\\frac{x-\\mu_P}{\\sigma_P})^2}, \\hspace{1cm} p^K(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_K}} e^{-(\\frac{x-\\mu_K}{\\sigma_K})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele obydwu klas, otrzymane dla rozważanego przykładu przy wykorzystaniu modelowania parametrycznego, są następujące:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PSY = np.array([40,7,25])\n",
    "KOTY = np.array([8,4,7])\n",
    "mu_P = np.mean(PSY)\n",
    "mu_K = np.mean(KOTY)\n",
    "s_P = np.std(PSY)\n",
    "s_K = np.std(KOTY)\n",
    "P = norm.pdf(x,mu_P,s_P)\n",
    "K = norm.pdf(x,mu_K,s_K)\n",
    "#plt.plot(x,P, color='blue')\n",
    "#plt.plot(x,K, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wzór Bayesa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podstawą rozważanej strategii klasyfikacji jest określenie p-stw przynależności próbki do każdej z rozważanych klas, a następnie wybór klasy, dla której było ono największe. Dla przedstawionego przykładu procedura klasyfikacji będzie miała następujący przebieg:\n",
    "-  algorytmowi prezentowana jest próbka x\n",
    "-  wyznaczana jest wartość $p(PIES/ x)$ i wartość $p(KOT/ x)$\n",
    "-  na podstawie relacji między wartościami p-stw podejmowana jest decyzja o przynależności nieznanej próbki do jednej z rozważanych klas\n",
    "\n",
    "W poprzedniej części przedstawione zostały metody wyznaczania modeli klas, a więc p-stw postaci $p(x /_{PIES})$ i $p(x /_{KOT})$. Aby dokonać klasyfikacji nieznanej próbki należy określić p-stwa $p(PIES/ x)$ i $p(KOT / x)$. Jak powiązać ze sobą prawdopodobieństwa warunkowe:  $p(KLASA / x)$ i $p(x / KLASA)$?\n",
    "\n",
    "Aby odpowiedzieć sobie na to pytanie, należy postawić pytanie bardziej podstawowe: czym jest prawdopodobieństwo warunkowe. P-stwo warunkowe jest definiowane jako:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ p(a/b) = \\frac{p(a,b)}{p(b)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(a,b)$ oznacza p-stwo łączne dla dwóch zdarzeń, $p(b)$ - zdarzenia, o którym wiemy że nastąpiło i które potencjalnie daje nam dodatkową wiedzę na temat możliwości zaistnienia zdarzenia $a$. Potencjalnie, bo w jednym przypadku, gdy zdarzenia $a$ i $b$ są od siebie niezależne, żadnej dodatkowej wiedzy nie zyskujemy (formalnie, niezależność zdarzeń to warunek $p(a,b) = p(a)p(b)$, co po podstawieniu do wzoru na p-stwo warunkowe ($p(a,b)/p(b) = p(a)p(b)/p(b)$) prowadzi do wniosku, że $p(a/b)=p(a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy, że wzór określający p-stwo warunkowe można zapisać jako:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ p(a,b) = p(a/b) p(b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "co w oczywisty sposób prowadzi do alternatywnej postaci:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ p(a,b) = p(b / a) p(a)  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i wreszcie do postaci wiążącej ze sobą p-stwa warunkowe dla obu wariantów, nazywanej  <font color=blue> **regułą Bayesa**</font>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ p(a / b)  = \\frac{p(b / a) \\cdot p(a)}{p(b)}   $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reguła Bayesa stanowi przepis na praktyczną realizację klasyfikacji danych. Wracając do przedstawionego wcześniej przykładu, jej wykorzystanie polega na obliczeniu wartości dwóch wyrażeń:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ p(PIES /_x)  = p(x /_{PIES}) \\cdot p(PIES) \\hspace{1cm} p(KOT /_x)  = p(x /_{KOT}) \\cdot p(KOT)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W obydwu wyrażeniach w porównaniu ze wzorem Bayesa pominięto obliczanie wartości składnika $p(b)$ (symbol $b$ wzoru ogólnego to symbol $x$ w przykładzie) który w obydwu przypadkach jest taki sam. Decyzja to wybór tej klasy, dla której wartość obliczonego wyrażenia jest wyższa.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wiedza niezbędna do dokonania klasyfikacji Bayesowskiej to określone na podstawie danych treningowych modele klas (w postaci parametrycznych lub nieparametrycznych rozkładów p-stw, np. $p(x/_{PIES}$ lub $p(x/_{KOT})$). Dodatkowo, na postawie danych treningowych musi zostać oszacowane p-stwo wystąpienia w zbiorze uczącym próbki rozważanej klasy (np $p(PIES)$ lub $p(KOT)$), nazywane p-stwem <font color=blue> **a priori** </font>). Wynikowe prawdopodobieństwo (np. $p(PIES/x)$ lub $p(KOT/x)$) jest nazywane p-stwem <font color=blue> **a posteriori** </font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przykład"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zastosujmy klasyfikację Bayesa w odniesieniu do przedstawionego wcześniej przykładu. Komponenty niezbędne dla zbudowania klasyfikatora Bayesa to rozkłady p-stw masy ciała osobników dla rozważanych klas ($p(x/PIES)$ i $(p(x/KOT)$) oraz p-stwa a priori wystąpienia osobnika danej klasy. Ponieważ próbek jest 6, 3 z nich to koty i 3 to psy, więc p-stwa a priori to $p(KOT)=1/2$ i $p(PIES)=1/2$. Załóżmy parametryczne (Gausowskie) rozkłady p-stw dla masy w obrębie obydwu klas i załóżmy, że celem jest dokonanie klasyfikacji próbki $x=9$. Odpowiednia procedura została przedstawiona w poniższym skrypcie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kot\n"
     ]
    }
   ],
   "source": [
    "# dane wejściowe\n",
    "PSY = np.array([40,6,25])\n",
    "KOTY = np.array([7,3,6])\n",
    "# obliczenie parametrów rozkładów próbek obydwu klas\n",
    "mu_P = np.mean(PSY)\n",
    "mu_K = np.mean(KOTY)\n",
    "s_P = np.std(PSY)\n",
    "s_K = np.std(KOTY)\n",
    "# p-stwa a priori\n",
    "a_P = len(PSY)/(len(PSY) + len(KOTY))\n",
    "a_K = 1 - a_P\n",
    "# próbka do klasyfikacji\n",
    "x = 9\n",
    "# obliczenia: p-stwa, że w klasach PIES, KOT znajdzie się próbka o wartości x\n",
    "p_P = norm.pdf(x,mu_P,s_P)\n",
    "p_K = norm.pdf(x,mu_K,s_K)\n",
    "# podjęcie decyzji, do której klasy należy x\n",
    "if p_P*a_P > p_K*a_K:\n",
    "    print('Pies')\n",
    "else:\n",
    "    print('Kot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załóżmy, że do używanego wcześniej wektora PSY dodamy trzy kolejne próbki, o identycznych wartościach jak poprzednio (zbiór przykładów klasy liczy teraz 6 elementów), zaś wektor KOTY pozostaje bez zmian (zawiera 3 elementy). Ponieważ próbek jest 9, 3 z nich to koty, 6 to psy, więc p-stwa a priori to $p(KOT)=1/3$ zaś $p(PIES)=2/3$. Ponownie, niech przedmiotem klasyfikacji będzie próbka $x=9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pies\n"
     ]
    }
   ],
   "source": [
    "PSY = np.array([40,6,25,40,6,25])\n",
    "mu_P = np.mean(PSY)\n",
    "s_P = np.std(PSY)\n",
    "a_P = len(PSY)/(len(PSY) + len(KOTY))\n",
    "a_K = 1 - a_P\n",
    "p_P = norm.pdf(x,mu_P,s_P)\n",
    "# podjęcie decyzji, do której klasy należy x\n",
    "if p_P*a_P > p_K*a_K:\n",
    "    print('Pies')\n",
    "else:\n",
    "    print('Kot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uzyskany wynik jest inny niż poprzednio, mimo że p-stwo $p(x/PIES)$ się nie zmieniło (zarówno wartość średnia jak i odchylenie standardowe próbek klasy pozostały takie same), zmieniły się p-stwa a priori. Uwzględnienie składnika a priori jest niezbędne, by decyzje były sensowne. Rozważmy nową klasę, obejmującą przykłady Diabłów Tasmańskich (niech klasa nazywa się DT). Masy Diabłów Tasmańskich są trochę większe od masy kotów, więc p-stwo $p(x=9/DT) > p(x=9/KOT), p(x=9/PIES)$. Mimo to, pojawienie się np. w Łodzi przedstawiciela tego gatunku jest dość mało prawdopodobne ($p(DT)=0.00001$). Dlatego wynik klasyfikacji Bayesa dla dwóch klas: PIES i DT nawet dla osobników o dowolnie małej wadze, wskaże na psa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasyfikacja Bayesa jest jedną z podstawowych strategii uczenia maszynowego, dlatego też prawie każdy pakiet procedur wspierających uczenie maszynowe zawiera odpowiednie narzędzia programowe jej realizacji. W bibliotece $\\tt{sklearn}$ znajduje się cały pakiet narzędzi do klasyfikacji Bayesowskiej. Podstawowy algrytm parametrycznej, Gaussowskiej metody klasyfikacji Bayesa realizuje obiekt <font color=green>$\\tt{GaussianNB}$</font> - *Gaussian Naive Bayes* (wyjaśnienie przymiotnika 'Naive' zostanie dokonane gdy omawiana będzie klasyfikacja danych wielowymiarowych). Realizacja klasyfikacji wcześniej używanych danych, z wykorzystaniem tego obiektu jest następująca:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.]\n",
      "Prawdopodbieństwa przynależności do klas: %f [[ 0.58953434  0.41046566]]\n",
      "A priori: %f [ 0.66666667  0.33333333]\n",
      "Wartości średnie: %f [[ 23.66666667]\n",
      " [  5.33333333]]\n",
      "Odchylenia: %f [[ 13.91242451]\n",
      " [  1.69967323]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "Dane = np.hstack((PSY,KOTY))\n",
    "Etykiety = np.ones(len(Dane))\n",
    "Etykiety[0:len(PSY)] = 0 # Klasa PSY ma etykietę 0, klasa KOTY etykietę 1\n",
    "x=9\n",
    "Klasyfikator = GaussianNB() # utworzenie obiektu \n",
    "Model = Klasyfikator.fit(Dane.reshape(-1, 1),Etykiety) # trening klasyfikatora ('reshape' wymagane, gdy dane 1D)\n",
    "print(Klasyfikator.predict(x))\n",
    "print('Prawdopodbieństwa przynależności do klas: %f',Klasyfikator.predict_proba(x))\n",
    "print('A priori: %f', Model.class_prior_)\n",
    "print('Wartości średnie: %f', Model.theta_)\n",
    "print('Odchylenia: %f', np.sqrt(Model.sigma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korzystanie z narzędzi biblioteki $\\tt{sklearn}$ odbywa się według ustalonego schematu, wspólnego dla większości dostępnych w niej algorytmów klasyfikacji, regresji czy grupowania. Pierwszy krok to wytrenowanie modelu, realizowany poprzez wywołanie funkcji <font color=green> $\\tt{fit(.)}$ </font>. Argumentami tej funkcji są zbiór próbek treningowych i zbiór etykiet, przy czym w macierzy danych próbki są umieszczone w kolejnych wierszach. Wynikiem wykonania metody jest zbudowanie modelu klasyfikatora, przy czym dla klasyfikatora Bayesowskiego z parametrycznymi funkcjami Gaussa, oznacza to określenie prawdopodobieństw a priori dla klas oraz estymację parametrów funkcji Gaussa (wartości średnich i wariancji) modelujących rozkłady p-stw w obrębie klas. Drugi krok procedury to wykorzystanie wytrenowanego klasyfikatora do wyznaczenia etykiety sprawdzanej próbki (lub wielu próbek). Jest to dokonywane z wykorzystaniem funkcji <font color=green> $\\tt{predict(.)}$ </font>, pobierającej jako argument próbkę, która zwraca etykietę zwycięskiej klasy. Wyznaczone p-stwa przynależności próbki do klas można uzyskać wywołując metodę <font color=green> $\\tt{predict\\_proba(.)}$ </font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 klasy ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
