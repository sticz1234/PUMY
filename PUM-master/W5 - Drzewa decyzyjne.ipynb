{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drzewa decyzyjne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Koncepcja drzew decyzyjnych to powiązanie pomysłu na klasyfikację danych wykorzystującą szczególny sposób podziału przestrzeni cech, z dążeniem do zapewnienia możliwości interpretacji przesłanek wykorzystywanych w podejmowaniu decyzji. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zakładając, że dane wejściowe są wektorami określonymi w $n$-wymiarowej przestrzeni cech, drzewo decyzyjne jest algorytmem, w którym dla danej próbki poddawanej analizie, wartości jej indywidualnych cech są, w określonej kolejności, konfrontowane z ustalonymi w procesie treningu drzewa progami, prowadząc do przypisania próbce etykiety, na którymś z etapów prowadzonej analizy (różnym dla różnych próbek). Proces podejmowania decyzji zaczyna się od analizy wybranej cechy, następnie rozwidla się w zależności od wyniku porównania wartości cechy z progiem, później może się znowu rozwidlać itd., aż do osiągnięcia stanu, w którym algorytm uważa, że 'wie' do jakiej klasy zakwalifikować próbkę. Proces ten graficznie odpowiada odwróconemu drzewu, którego korzeniem jest pierwsze sprawdzenie, zawierającym hierarchicznie umieszczone kolejne węzły, zakończone liśćmi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przykład\n",
    "Załóżmy, że zbudowane zostało drzewo decyzyjne, przeznaczone do identyfikacji znaków drogowych, występujących w obszarach analizowanego przez algorytm obrazu. Załóżmy też, że obiekt obszaru został opisany (za pomocą odpowiednich metod przetwarzania obrazu) za pomocą zbioru cech, takich jak 'geometria' (cecha mierzona proporcją długości konturu do jego powierzchni) - $x_1$, nasycenie barwy tła (mierzona wartością komponentu S przestrzeni HSV) - $x_2$, liczba obiektów geometrycznych na obszarze tła - $x_3$, itd, a więc opisem obiektu staje się wektor o strukturze $\\mathbf{x} = [ 2.1, 10, 0, ...]$. Niech analiza nieznanej próbki $\\mathbf{x} = [ x_1, x_2, x_3 ...]$  rozpoczyna się  od sprawdzenia 'geometrii'. Załóżmy, że wartość progowa dla testu wynosi $2.5$ (zauważmy, że proporcja powierzchni do obwodu dla różnych figur geometrycznych jest różna: dla koła wynosi $2/r$, kwadratu - $4/r$, trójkąta - $4 \\sqrt{3}/r$, więc zakładając normalizację rozmiaru: r=1 można znaleźć wartość progową pozwalającą na separację różnych kształtów). Zależnie od podjętej decyzji, następuje rozwidlenie i przejście do sprawdzenia kolejnej cechy, w kolejnym węźle drzewa. Dla decyzji 'koło' (podjętej w rozważanym przypadku), załóżmy, że kolejne  sprawdzenie testuje ile obiektów znajduje się w obszarze tła. Jeżeli przyjmiemy, że próg decyzyjny to wartość 0.5, wtedy, jeśli wartość cechy jest mniejsza, kolejnym węzłem hipotetycznego, wytrenowanego drzewa jest liść - znak: 'zakaz ruchu'. Ten przypadek zachodzi w przedstawionym przykładzie, dla którego wynik klasyfikacji będzie ustalony po dokonaniu dwóch sprawdzeń. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając wytrenowane drzewo, jesteśmy w stanie prześledzić kryteria, jakie zostały zastosowane do podjęcia decyzji, więc ta metoda klasyfikacji ma ogromny walor 'rozumienia' rozwiązywanego problemu (tzw. 'wytłumaczalna sztuczna inteligencja' - ang. explainable AI). Sama procedura klasyfikacji jest też bardzo przejrzysta i prosta do implementacji. Można zauważyć, że klasyfikacja dokonywana w drzewie decyzyjnym jest w istocie klasyfikacją, wykorzystującą podział przestrzeni cech hiperpłaszczyznami prostopadłymi do osi przestrzeni cech i umieszczanymi w wartościach progów. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie drzewa decyzyjnego\n",
    "\n",
    "Podstawowym zadaniem klasyfikacji metodą drzew decyzyjnych jest określenie algorytmu treningu drzewa, a więc, algorytmu, którego wynikiem będzie określenie struktury drzewa (jakie cechy mają być sprawdzane w kolejnych węzłach) i określenie wartości progów, kierunkujących proces poszukiwań. Celem przyświecającym budowie drzewa jest zapewnienie maksymalnej poprawności klasyfikacji dzięki kolejnym podziałom populacji próbek na podzbiory, zawierające elementy o podobnych do siebie właściwościach, co jest istotą przynależności do tej samej kategorii. Dlatego też, struktura drzewa powinna promować te kolejne podziały na podzbiory, które najlepiej separują próbki różnych klas od próbek tej samej klasy. Odwzierciedleniem tej zasady są kryteria mierzące wzrost stopnia właściwej segregacji próbek: na każdym etapie budowy drzewa wybierana będzie cecha, która zapewni najlepszą możliwą segregację. Dwa podstawowe ilościowe kryteria oceny jakości segregacji próbek, stosowane w algorytmach budowy drzew decyzyjnych to:\n",
    "-  współczynnik Giniego 'nieczystości' podzbioru (Gini impurity index)\n",
    "-  entropia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorytm CART\n",
    "\n",
    "Algorytm CART  (Classification and Regression Tree) wykorzystuje współczynnik Giniego jako miarę jakości podziału próbek na zbiory. Współczynnik Giniego określający stopień jednorodności zbioru $S_i$, zawierającego próbki należące do $k$-klas jest określony jako: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "G(S_i) = 1 - \\sum_{j=1}^k p^2(j/S_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gdzie $p(j/S_i)$ oznacza p-stwo przynależności próbek podzbioru do klasy $j$. Współczynnik Giniego mierzy p-stwo błędnej klasyfikacji próbki. Kryterium wyboru atrybutu (cechy) zapewniającej najlepszy podział oryginalnego zbioru na podzbiory to zapewnienie maksymalizacji 'zysku', określonego jako różnica między wsp. Giniego obliczanym dla zbioru przed podziałem i ważoną sumą wsp. Giniego dla zbiorów po podziale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przykład\n",
    "\n",
    "Załóżmy, że dane dla podejmowania decyzji o przyznaniu lub odmowie kredytu, dokonywane na podstawie płci i dochodów aplikanta i pochodzące z wcześniej rozpatrywanych spraw są następujące: decyzja pozytywna - [M, Ś], [M, W] [K, W], decyzja negatywna: [K, Ś], [K, N], gdzie M,K oznacza mężczyznę / kobietę, a N, Ś, W to odpowiednio zarobki niskie, średnie i wysokie. Uczenie drzewa decyzyjnego dla tych danych zaczyna się od wyboru pierwszej sprawdzanej cechy (korzenia drzewa), a kryterium w algorytmie CART to zysk Giniego. Jeżeli rozważaną cechą jest płeć, współczynnik Giniego przed dokonaniem podziału to: $G^-=1 - (\\frac{3}{5})^2 - (\\frac{2}{5})^2 = \\frac{12}{25}$, zaś po podziale, to: $G^+_{Płeć}= \\frac{2}{5}(1-1) + \\frac{3}{5}(1-(\\frac{1}{3})^2-(\\frac{2}{3})^2) = \\frac{3}{5}\\frac{4}{9} = \\frac{4}{15}$. Jeżeli podział byłby dokonany z wykorzystaniem zarobków, współczynnik Giniego po podziale to: $G^+_{Zarobki}= \\frac{1}{5}(1-1) + \\frac{2}{5}(1-2(\\frac{1}{2})^2) + \\frac{2}{5}(1-1) = \\frac{2}{5} \\frac{1}{2} = \\frac{1}{5}$. Zysk Giniego w pierwszym przypadku jest mniejszy niż w drugim, dlatego też, jako pierwsza cecha wybierane są zarobki. Z trzech węzłów potomnych, dwa to liście, więc dalszej analizie zostaje poddany tylko trzeci, odpowiadający średnim zarobkom (analogiczna analiza prowadzi do wyboru płci, jako sprawdzanej w tym węźle cechy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[[0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "# sztuczne dane 'kredytowe': cecha '0' to płeć (M,K), cecha '1' to dochód \n",
    "X = [[0, 2000], [0, 3500], [0, 4500], [0, 5500], [1, 1800], [1, 3600], [1, 4100], [1, 4900]]\n",
    "# decyzje: 0 to nieprzyznany kredyt, 1 to przyznany kredyt\n",
    "Y = [0, 1, 1, 1, 0, 0, 1, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "# wizualizacja drzewa\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"Kredyty\")\n",
    "print(clf.predict([[0,3000],[1,3000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorytm ID3\n",
    "\n",
    "Algorytm ID3 (Iterative Dichotomizer) wykorzystuje entropię jako miarę jednorodności zbioru:\n",
    "\\begin{equation}\n",
    "H(s_i) = - \\sum_{j=1}^k p(j) \\log p(j)\n",
    "\\end{equation}\n",
    "gdzie $k$ oznacza liczbę klas, a podstawa logarytmu to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przykład\n",
    "\n",
    "Załóżmy, że danych jest $n$ próbek. Jeżeli wszystkie należą do jednej klasy, entropia zbioru to $H = -1 \\cdot \\log(1) = 0$. Jeżeli połowa należy do klasy A, zaś połowa do klasy B, to entropia $H = -0.5 \\cdot \\log \\frac{1}{2} - 0.5 \\cdot \\log \\frac{1}{2}= 1$. Jeżeli jedna czwarta należy do klasy A, pozostałe do B to: $H = -0.25 \\cdot \\log \\frac{1}{4} - 0.75 \\cdot \\log \\frac{3}{4} \\approx 0.5 + 0.3 = 0.8$. Wreszcie, jeśli próbki należą do 4 klas i każda z nich jest jednakowo prawdopodobna, to $H = 4 \\cdot(-0.25 \\cdot \\log \\frac{1}{4})= 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zasada tworzenia drzewa jest podobna jak w algorytmie CART - różnica polega na innej funkcji kryterialnej oceny cechy. Tak jak poprzednio, na każdym etapie algorytmu wybierana jest cecha zapewniająca najlepszy podział zbioru próbek. Miarą jakości jest różnica między entropią przed i po podziale, nazywana zyskiem informacyjnym (information gain): $IG = H_{k+1} - H_k$ . Entropia po podziale to średnia ważona entropii dla wynikowych podzbiorów: $ H_{k+1} = \\sum_j p(j)H^j_{k+1}$ gdzie $p(j)$ to p-stwo wylosowania podzbioru $j$ (czyli proporcja próbek, należących do tego podzbioru - $n_j/n$). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
